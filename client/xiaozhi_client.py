#!/usr/bin/env python3
"""
å°æ™ºè¯­éŸ³å¯¹è¯å®¢æˆ·ç«¯ â€” è¿è¡Œåœ¨è€ä¸‰(æ ‘è“æ´¾)ä¸Š
å”¤é†’è¯: "æ‚Ÿç©ºæ‚Ÿç©º" (sherpa-onnx KeywordSpotter)
ä¿æŒ WebSocket é•¿è¿æ¥ï¼Œæ£€æµ‹å”¤é†’è¯åå¼€å§‹å½•éŸ³å¯¹è¯

å‚è€ƒ: py-xiaozhi é¡¹ç›®åè®®å®ç°
"""

import argparse
import asyncio
import json
import logging
import os
import subprocess
import sys
import threading
import time
import requests
# import uuid

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
log = logging.getLogger("xiaozhi")

# ============================================================
#  é…ç½®
# ============================================================
SAMPLE_RATE = 16000
CHANNELS = 1
FRAME_DURATION_MS = 60
FRAME_SIZE = SAMPLE_RATE * FRAME_DURATION_MS // 1000  # 960
AUDIO_PLAY = "plughw:3,0"
AUDIO_REC = "plughw:2,0"
WAKE_WORD = "å¤šå¤š"
SHERPA_ASR_DIR = os.path.join(os.path.dirname(__file__), "models", "sherpa-onnx-streaming-zipformer-small-bilingual-zh-en-2023-02-16", "96")

def _contains_wake(text):
    """å”¤é†’è¯åŒ¹é…ï¼šå¤šå¤šï¼ˆå®¹é”™ï¼‰"""
    t = "".join(ch for ch in text if not ch.isspace())
    if "å¤šå¤š" in t:
        return True
    # å®¹é”™ï¼šå¤š/å“† è¿ç»­ä¸¤æ¬¡
    for i in range(len(t) - 1):
        if t[i] in ("å¤š", "å“†") and t[i + 1] in ("å¤š", "å“†"):
            return True
    return False

# ============================================================
#  Opus
# ============================================================
import opuslib_next as opuslib
_encoder = opuslib.Encoder(SAMPLE_RATE, CHANNELS, opuslib.APPLICATION_VOIP)
_decoder = opuslib.Decoder(SAMPLE_RATE, CHANNELS)

def pcm_to_opus(pcm: bytes) -> bytes:
    return _encoder.encode(pcm, FRAME_SIZE)

def opus_to_pcm(data: bytes) -> bytes:
    return _decoder.decode(data, FRAME_SIZE)

# ============================================================
#  æœ¬åœ° TTS
# ============================================================
def speak(text: str):
    subprocess.run(
        f'espeak -v zh -s 320 --stdout "{text}" | aplay -D {AUDIO_PLAY} -q',
        shell=True, stderr=subprocess.DEVNULL
    )

def speak_async(text: str):
    threading.Thread(target=speak, args=(text,), daemon=True).start()

# ============================================================
#  å”¤é†’è¯æ£€æµ‹ (sherpa-onnx æµå¼ASR + æ¨¡ç³ŠåŒ¹é…)
# ============================================================
class WakeWordListener:
    def __init__(self, device=AUDIO_REC):
        import sherpa_onnx
        import numpy as np
        self.sherpa_onnx = sherpa_onnx
        self.np = np
        self.device = device
        self.paused = False
        self.active = True
        self._proc = None
        self._cooldown = 2.0
        self._last_detect = 0

        encoder = os.path.join(SHERPA_ASR_DIR, "encoder-epoch-99-avg-1.onnx")
        decoder = os.path.join(SHERPA_ASR_DIR, "decoder-epoch-99-avg-1.onnx")
        joiner = os.path.join(SHERPA_ASR_DIR, "joiner-epoch-99-avg-1.onnx")
        tokens = os.path.join(SHERPA_ASR_DIR, "tokens.txt")

        log.info(f"åŠ è½½ sherpa-onnx æµå¼ASR: {SHERPA_ASR_DIR}")
        self.recognizer = sherpa_onnx.OnlineRecognizer.from_transducer(
            encoder=encoder, decoder=decoder, joiner=joiner, tokens=tokens,
            num_threads=4, sample_rate=SAMPLE_RATE, feature_dim=80, provider="cpu",
            enable_endpoint_detection=True,
            rule1_min_trailing_silence=2.4,
            rule2_min_trailing_silence=1.2,
            rule3_min_utterance_length=300,
        )
        log.info("sherpa-onnx æµå¼ASR åŠ è½½å®Œæˆ")

    def start(self, on_wake):
        threading.Thread(target=self._listen, args=(on_wake,), daemon=True).start()

    def _listen(self, on_wake):
        chunk_samples = int(SAMPLE_RATE * 0.1)  # 100ms
        chunk_bytes = chunk_samples * 2
        read_count = 0
        last_text = ""

        while self.active:
            # ç­‰å¾…éæš‚åœçŠ¶æ€
            if self.paused:
                time.sleep(0.1)
                continue

            # å¯åŠ¨/é‡å¯ arecord
            if self._proc is None or self._proc.poll() is not None:
                self._proc = subprocess.Popen(
                    ["arecord", "-D", self.device, "-f", "S16_LE",
                     "-r", str(SAMPLE_RATE), "-c", "1", "-t", "raw", "-q"],
                    stdout=subprocess.PIPE, stderr=subprocess.DEVNULL
                )
                stream = self.recognizer.create_stream()
                last_text = ""
                log.info(f"ğŸ‘‚ ç›‘å¬å”¤é†’è¯: {WAKE_WORD}")

            data = self._proc.stdout.read(chunk_bytes)
            if not data:
                time.sleep(0.05)
                continue

            read_count += 1
            if read_count % 300 == 0:
                log.info(f"[debug] audio chunks: {read_count}, paused={self.paused}")

            if self.paused:
                continue

            samples = self.np.frombuffer(data, dtype=self.np.int16).astype(self.np.float32) / 32768.0
            stream.accept_waveform(SAMPLE_RATE, samples)
            while self.recognizer.is_ready(stream):
                self.recognizer.decode_stream(stream)
            text = self.recognizer.get_result(stream).strip()
            if text and text != last_text:
                log.info(f"asr: {text}")
                last_text = text
                if _contains_wake(text):
                    self._trigger(on_wake, text, stream)
            if self.recognizer.is_endpoint(stream):
                if text:
                    log.info(f"asr final: {text}")
                    if _contains_wake(text):
                        self._trigger(on_wake, text, stream)
                self.recognizer.reset(stream)
                last_text = ""

    def _trigger(self, on_wake, text, stream):
        now = time.time()
        if now - self._last_detect < self._cooldown:
            return
        self._last_detect = now
        log.info(f"ğŸ¯ å”¤é†’è¯! ({text})")
        self.recognizer.reset(stream)
        on_wake()

    def pause(self):
        self.paused = True
        # åœæ‰ arecord é‡Šæ”¾è®¾å¤‡ï¼Œè®©å½•éŸ³å¯¹è¯å¯ä»¥ç”¨
        if self._proc:
            try:
                self._proc.terminate()
                self._proc.wait(timeout=2)
            except Exception:
                pass
            self._proc = None

    def resume(self):
        self.paused = False

    def stop(self):
        self.active = False
        if self._proc:
            self._proc.terminate()

# ============================================================
#  å°æ™ºå®¢æˆ·ç«¯ (é•¿è¿æ¥)
# ============================================================
class PalmGestureListener:
    """é€šè¿‡ face_tracker çš„ /api/status è½®è¯¢æ‰‹æŒçŠ¶æ€ï¼šå‡ºç°å¼€å§‹å½•éŸ³ï¼Œæ¶ˆå¤±ç»“æŸå½•éŸ³"""

    def __init__(self, status_url="http://127.0.0.1:5000/api/status", interval=0.2, hold_seconds=0.8):
        self.status_url = status_url
        self.interval = interval
        self.hold_seconds = hold_seconds
        self.active = True
        self._last_palm_ts = 0.0
        self._recording = False

    def start(self, on_palm_start, on_palm_end):
        def _run():
            log.info("âœ‹ æ‰‹æŒè§¦å‘æ¨¡å¼å·²å¯ç”¨ï¼ˆå¼ æ‰‹å¼€å§‹ï¼Œæ”¾ä¸‹ç»“æŸï¼‰")
            while self.active:
                palm = False
                try:
                    r = requests.get(self.status_url, timeout=0.5)
                    if r.ok:
                        st = r.json()
                        g = (st.get("gesture") or {}).get("gesture", "none")
                        palm = (g == "open_palm")
                except Exception:
                    pass

                now = time.time()
                if palm:
                    self._last_palm_ts = now
                    if not self._recording:
                        self._recording = True
                        on_palm_start()
                else:
                    # æ‰‹æŒæ¶ˆå¤±æŒç»­ä¸€æ®µæ—¶é—´ååœæ­¢ï¼Œé¿å…æŠ–åŠ¨
                    if self._recording and (now - self._last_palm_ts) > self.hold_seconds:
                        self._recording = False
                        on_palm_end()

                time.sleep(self.interval)

        threading.Thread(target=_run, daemon=True).start()

    def stop(self):
        self.active = False


class XiaozhiClient:
    def __init__(self, ws_url: str, device_id: str):
        self.ws_url = ws_url
        self.device_id = device_id
        self.session_id = None
        self.ws = None
        self.connected = False
        self.is_speaking = False  # æœåŠ¡ç«¯åœ¨è¯´è¯
        self.is_listening = False  # æ­£åœ¨å½•éŸ³
        self._rec_proc = None
        self._play_proc = None
        self._loop = None
        self._send_task = None

    async def connect(self):
        """å»ºç«‹é•¿è¿æ¥"""
        import websockets
        headers = {
            "Device-Id": self.device_id,
            "Client-Id": self.device_id,
            "Protocol-Version": "1",
        }
        log.info(f"ğŸ”— è¿æ¥ {self.ws_url}")
        try:
            self.ws = await websockets.connect(
                self.ws_url, max_size=None,
                additional_headers=headers,
                ping_interval=20, ping_timeout=20,
                close_timeout=10,
            )
        except TypeError:
            self.ws = await websockets.connect(
                self.ws_url, max_size=None,
                extra_headers=headers,
                ping_interval=20, ping_timeout=20,
                close_timeout=10,
            )
        # å‘ hello
        hello = {
            "type": "hello",
            "version": 1,
            "transport": "websocket",
            "device_id": self.device_id,
            "device_name": "è€ä¸‰-æ ‘è“æ´¾",
            "features": {"mcp": False},
            "audio_params": {
                "format": "opus",
                "sample_rate": SAMPLE_RATE,
                "channels": CHANNELS,
                "frame_duration": FRAME_DURATION_MS,
            },
        }
        await self.ws.send(json.dumps(hello))
        log.info("ğŸ“¤ å·²å‘é€ hello")

        # ç­‰ hello å“åº”
        resp = await asyncio.wait_for(self.ws.recv(), timeout=10)
        try:
            data = json.loads(resp)
            self.session_id = data.get("session_id", "")
            log.info(f"âœ… è¿æ¥æˆåŠŸ, session: {self.session_id}")
            self.connected = True
        except Exception as e:
            log.error(f"æ¡æ‰‹å¤±è´¥: {e}, resp: {str(resp)[:200]}")
            return False
        return True

    async def message_loop(self):
        """æŒç»­æ¥æ”¶æ¶ˆæ¯"""
        try:
            async for message in self.ws:
                if isinstance(message, bytes):
                    # Opus éŸ³é¢‘ â†’ è§£ç æ’­æ”¾
                    self._audio_count = getattr(self, '_audio_count', 0) + 1
                    if self._audio_count <= 3 or self._audio_count % 50 == 0:
                        log.info(f"ğŸ”ˆ æ”¶åˆ°éŸ³é¢‘å¸§ #{self._audio_count}, {len(message)} bytes")
                    try:
                        pcm = opus_to_pcm(message)
                        self._play_pcm(pcm)
                    except Exception as e:
                        log.error(f"éŸ³é¢‘è§£ç /æ’­æ”¾é”™è¯¯: {e}")
                else:
                    data = json.loads(message)
                    await self._handle(data)
        except Exception as e:
            log.error(f"è¿æ¥æ–­å¼€: {e}")
            self.connected = False

    async def _handle(self, msg: dict):
        t = msg.get("type", "")
        if t == "tts":
            state = msg.get("state", "")
            if state == "start":
                self.is_speaking = True
                log.info("ğŸ”Š æœåŠ¡ç«¯å¼€å§‹è¯´è¯")
            elif state == "sentence_start":
                log.info(f"ğŸ’¬ {msg.get('text', '')}")
            elif state == "stop":
                self.is_speaking = False
                log.info("ğŸ”Š æœåŠ¡ç«¯è¯´è¯ç»“æŸ")
                # åœæ­¢å½•éŸ³ï¼Œè®©å”¤é†’è¯ç›‘å¬æ¢å¤
                if self.is_listening:
                    await self.stop_listening()
        elif t == "stt":
            log.info(f"ğŸ¤ è¯†åˆ«: {msg.get('text', '')}")
        elif t == "llm":
            log.info(f"ğŸ¤– {msg.get('text', '')}")
        elif t == "hello":
            self.session_id = msg.get("session_id", self.session_id)
            log.info(f"hello å“åº”, session: {self.session_id}")

    def _play_pcm(self, pcm: bytes):
        """ç›´æ¥å†™å…¥ aplay è¿›ç¨‹"""
        try:
            if self._play_proc is None or self._play_proc.poll() is not None:
                log.info(f"ğŸ”Š å¯åŠ¨ aplay è¿›ç¨‹ (è®¾å¤‡: {AUDIO_PLAY}, é‡‡æ ·ç‡: {SAMPLE_RATE})")
                self._play_proc = subprocess.Popen(
                    ["aplay", "-D", AUDIO_PLAY, "-f", "S16_LE",
                     "-r", str(SAMPLE_RATE), "-c", "1", "-q"],
                    stdin=subprocess.PIPE, stderr=subprocess.PIPE
                )
            self._play_proc.stdin.write(pcm)
            self._play_proc.stdin.flush()
        except Exception as e:
            log.error(f"æ’­æ”¾é”™è¯¯: {e}")
            if self._play_proc:
                err = self._play_proc.stderr.read(200) if self._play_proc.stderr else b""
                log.error(f"aplay stderr: {err}")
            self._play_proc = None

    async def on_wake_word(self):
        """å”¤é†’è¯è§¦å‘ï¼šå¼€å§‹ä¸€è½®å¯¹è¯"""
        if not self.connected:
            log.warning("æœªè¿æ¥ï¼Œå¿½ç•¥å”¤é†’")
            return
        if self.is_listening:
            return

        log.info(f"ğŸ™ï¸ å”¤é†’è¯è§¦å‘ï¼Œå¼€å§‹å¯¹è¯: {WAKE_WORD}")

        if self.is_speaking:
            abort = {"session_id": self.session_id, "type": "abort", "reason": "wake_word_detected"}
            await self.ws.send(json.dumps(abort))
            if self._play_proc:
                try:
                    self._play_proc.terminate()
                except Exception:
                    pass
                self._play_proc = None

        # æœ¬åœ°åº”ç­”ï¼šæ”¶åˆ°å”¤é†’è¯åå…ˆè¯´â€œæˆ‘åœ¨â€
        speak_async("æˆ‘åœ¨")

        # å…³é”®ä¿®å¤ï¼šä¸å‘é€ detect(text=å”¤é†’è¯)ï¼Œé¿å…æœåŠ¡ç«¯æŠŠå”¤é†’è¯å½“é—®é¢˜
        # ç­‰æœ¬åœ°â€œæˆ‘åœ¨â€æ’­å®Œå†å¼€å§‹æ”¶æŒ‡ä»¤
        await asyncio.sleep(0.9)

        start = {
            "session_id": self.session_id,
            "type": "listen",
            "state": "start",
            "mode": "auto",
        }
        await self.ws.send(json.dumps(start))

        self.is_listening = True
        self._send_task = asyncio.create_task(self._record_and_send())

    async def _record_and_send(self):
        """å½•éŸ³å¹¶é€šè¿‡ WebSocket å‘é€ Opus å¸§"""
        frame_bytes = FRAME_SIZE * 2
        self._rec_proc = subprocess.Popen(
            ["arecord", "-D", AUDIO_REC, "-f", "S16_LE",
             "-r", str(SAMPLE_RATE), "-c", "1", "-t", "raw", "-q"],
            stdout=subprocess.PIPE, stderr=subprocess.DEVNULL
        )
        loop = asyncio.get_event_loop()
        log.info("ğŸ™ï¸ å½•éŸ³ä¸­...")
        # ä¸¢å¼ƒèµ·å§‹çº¦0.7ç§’ï¼Œé¿å…æŠŠå”¤é†’è¯å°¾éŸ³å½“æˆç”¨æˆ·æŒ‡ä»¤
        warmup_frames = 12
        sent_frames = 0
        try:
            while self.is_listening and self.connected:
                data = await loop.run_in_executor(None, self._rec_proc.stdout.read, frame_bytes)
                if len(data) == frame_bytes:
                    if warmup_frames > 0:
                        warmup_frames -= 1
                        continue
                    opus = pcm_to_opus(data)
                    await self.ws.send(opus)
                    sent_frames += 1
        except Exception as e:
            log.error(f"å½•éŸ³å‘é€é”™è¯¯: {e}")
        finally:
            self._stop_recording()

    def _stop_recording(self):
        self.is_listening = False
        if self._rec_proc:
            try:
                self._rec_proc.terminate()
            except Exception:
                pass
            self._rec_proc = None

    async def stop_listening(self):
        """åœæ­¢å½•éŸ³"""
        self._stop_recording()
        if self._send_task:
            self._send_task.cancel()
        stop = {"session_id": self.session_id, "type": "listen", "state": "stop"}
        try:
            await self.ws.send(json.dumps(stop))
        except Exception:
            pass
        log.info("ğŸ™ï¸ åœæ­¢å½•éŸ³")


# ============================================================
#  ä¸»ç¨‹åº
# ============================================================
async def main(ws_url: str):
    # ä½¿ç”¨å›ºå®š device_idï¼Œé¿å…æœåŠ¡ç«¯æŠŠåŒä¸€è®¾å¤‡å½“æ–°è®¾å¤‡å¯¼è‡´é…ç½®æ¼‚ç§»
    device_id = "pi-laosan-001"
    log.info(f"è®¾å¤‡ID: {device_id}")

    client = XiaozhiClient(ws_url, device_id)

    # è¿æ¥
    if not await client.connect():
        log.error("è¿æ¥å¤±è´¥ï¼Œé€€å‡º")
        return

    speak_async("å°æœºå™¨äººä¸Šçº¿äº†")

    # å”¤é†’è¯ç›‘å¬
    listener = WakeWordListener()
    loop = asyncio.get_event_loop()

    def on_wake():
        listener.pause()
        asyncio.run_coroutine_threadsafe(client.on_wake_word(), loop)

        def wait_and_resume():
            # é˜²å¡æ­»ï¼šæœ€å¤šç­‰å¾…25ç§’ï¼Œè¶…æ—¶ä¹Ÿå¼ºåˆ¶æ¢å¤ç›‘å¬
            start_ts = time.time()
            time.sleep(2)
            while True:
                if not client.is_listening and not client.is_speaking:
                    break
                if time.time() - start_ts > 25:
                    log.warning("æ¢å¤ç›‘å¬ç­‰å¾…è¶…æ—¶ï¼Œå¼ºåˆ¶æ¢å¤")
                    break
                time.sleep(0.5)
            time.sleep(0.8)
            listener.resume()
            log.info(f"ğŸ‘‚ ç»§ç»­ç›‘å¬: {WAKE_WORD}")

        threading.Thread(target=wait_and_resume, daemon=True).start()

    listener.start(on_wake)

    # æ¶ˆæ¯å¾ªç¯ï¼ˆä¿æŒé•¿è¿æ¥ï¼‰
    await client.message_loop()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å°æ™ºè¯­éŸ³å¯¹è¯å®¢æˆ·ç«¯")
    parser.add_argument("--ws", default="ws://192.168.0.69:8100/xiaozhi/v1/")
    parser.add_argument("--play-device", default=AUDIO_PLAY)
    parser.add_argument("--rec-device", default=AUDIO_REC)
    args = parser.parse_args()

    AUDIO_PLAY = args.play_device
    AUDIO_REC = args.rec_device

    asyncio.run(main(args.ws))
